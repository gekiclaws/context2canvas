{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed42f10-f71b-4a43-bc09-ca1354f88d20",
   "metadata": {},
   "source": [
    "## Context2Canvas - AI ggplot2 Code Generation\n",
    "\n",
    "Brief Elevator Pitch of our project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa12a04-3acf-43f0-b984-d99790d4afff",
   "metadata": {},
   "source": [
    "## Setup the functions for prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04c17cf-85f5-4920-bea1-db9e480a4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def prompt_model(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        store=True,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", 'content': prompt}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1063fc8f-0d55-499d-a112-2d9752b77821",
   "metadata": {},
   "source": [
    "## Chunk data and generate indices (setting up vector database)\n",
    "\n",
    "User queries will be matched to indexes that best approximate the text chunks used to summarize an answer. For this assignment, you may chunk the text and then prompt the model to generate questions that are answerable by the text. The generated questions can then be used as the \"documents\" stored in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b74904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "\n",
    "import json\n",
    "with open('annotations.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b60f00b-27e7-41c4-ba3b-89ddb9dc3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import langchain chunking library\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "#from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "#we can use better parsing techniques in a future update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90f66807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up text splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8304580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk data\n",
    "annotations = [str(record[\"general_figure_info\"]) for record in data]\n",
    "types = [record[\"type\"] for record in data]\n",
    "\n",
    "print(len(annotations) == len(types))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce91801-86ab-43ae-9ff1-1cd0891e82e5",
   "metadata": {},
   "source": [
    "## Build the vector database\n",
    "\n",
    "When building the vector database, be sure to maintain a mapping between the generated questions and the chunks that can be used later to retrieve the chunks from the most similar indices to the user query provided.\n",
    "\n",
    "You may also add the function to query the vector database that you will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c65e598a-5463-4237-8de2-74a19981b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the collection\n",
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"c2c\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913aa98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add data to collection\n",
    "collection.add(\n",
    "    documents = annotations[:1000], \n",
    "    ids = [str(i) for i in range(len(annotations[:1000]))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83399cf",
   "metadata": {},
   "source": [
    "## Analyze Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79308151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect user input data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#filepath = input(\"insert filepath of the dataset you want to analyze\")\n",
    "df = pd.read_csv(\"pixar_films.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the column names associated with this dataset?\n",
    "columns = {col: str(dtype) for col, dtype in df.dtypes.items()}\n",
    "\n",
    "#generate summary statistics\n",
    "summaryStats = df.describe()\n",
    "\n",
    "# Create Interesting data questions based on the variables + summary statistics\n",
    "question = prompt_model(f\"create an interesting data question based on {columns} and {summaryStats}. Do not return anything besides the data question.\")\n",
    "\n",
    "# gather visualization type from data question\n",
    "vizType = prompt_model(f\"what is the best visualization class we should use to characterize this problem, given {question}, {columns}, and {summaryStats}? Do not return anything besides the visualization type. Only return a type listed in {supportedClasses}\")\n",
    "print(vizType)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f920870",
   "metadata": {},
   "source": [
    "## Use VectorDB to query similar examples for few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5127bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = collection.query(\n",
    "    query_texts= [question], # Chroma will embed this for you\n",
    "    n_results=3 # how many results to return\n",
    ")\n",
    "print(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b30541-75ca-4647-86b3-4af98d395c31",
   "metadata": {},
   "source": [
    "## Generate the graph's code based on the collected examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483d9e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeOutput = prompt_model(f\"generate python code for this visualization, given {vizType}, {question}, {columns}, and {summaryStats}\")\n",
    "print(codeOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5636a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python code with vector databse helper hints\n",
    "codeOutput = prompt_model(f\"generate python code for this visualization, given {vizType}, {question}, {columns}, and {summaryStats}. Model your output on the examples given by {examples}\")\n",
    "print(codeOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6554f8ce",
   "metadata": {},
   "source": [
    "## Analysis of C2C's Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7176779e-12a3-4d39-bfc9-91d5f6ae6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "\n",
    "scorer = BERTScorer(model_type='bert-base-uncased')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
