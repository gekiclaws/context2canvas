{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed42f10-f71b-4a43-bc09-ca1354f88d20",
   "metadata": {},
   "source": [
    "## Context2Canvas - AI ggplot2 Code Generation\n",
    "\n",
    "Brief Elevator Pitch of our project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa12a04-3acf-43f0-b984-d99790d4afff",
   "metadata": {},
   "source": [
    "## Setup the functions for prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04c17cf-85f5-4920-bea1-db9e480a4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def prompt_model(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        store=True,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", 'content': prompt}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1063fc8f-0d55-499d-a112-2d9752b77821",
   "metadata": {},
   "source": [
    "## Chunk data and generate indices (setting up vector database)\n",
    "\n",
    "User queries will be matched to indexes that best approximate the text chunks used to summarize an answer. For this assignment, you may chunk the text and then prompt the model to generate questions that are answerable by the text. The generated questions can then be used as the \"documents\" stored in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b74904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "\n",
    "import json\n",
    "with open('annotations.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b60f00b-27e7-41c4-ba3b-89ddb9dc3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import langchain chunking library\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "#from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "#we can use better parsing techniques in a future update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90f66807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up text splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8304580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#chunk data\n",
    "annotations = [str(record[\"general_figure_info\"]) for record in data]\n",
    "types = [record[\"type\"] for record in data]\n",
    "\n",
    "print(len(annotations) == len(types))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce91801-86ab-43ae-9ff1-1cd0891e82e5",
   "metadata": {},
   "source": [
    "## Build the vector database\n",
    "\n",
    "When building the vector database, be sure to maintain a mapping between the generated questions and the chunks that can be used later to retrieve the chunks from the most similar indices to the user query provided.\n",
    "\n",
    "You may also add the function to query the vector database that you will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e598a-5463-4237-8de2-74a19981b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the collection\n",
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"c2c\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913aa98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add data to collection\n",
    "collection.add(\n",
    "    documents = annotations[:1000], \n",
    "    ids = [str(i) for i in range(len(annotations[:1000]))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83399cf",
   "metadata": {},
   "source": [
    "## Analyze Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = #data inserted by user\n",
    "\n",
    "# what are the variables associated with this data?\n",
    "prompt_model(\"What are the variables associated with this dataset? Return them as a python dictionary, in the format of 'variable_name: data_type'. Do not return any dialogue besides the python dictionary.\")\n",
    "\n",
    "#generate summary statistics\n",
    "prompt_model(f\"using {input_data}, generate me summary statistics of each variable according to these {standards}\")\n",
    "\n",
    "# Create Interesting data questions based on the variables + summary statistics\n",
    "prompt_model(f\"using this process for creating data questions, create an interesting data question based on the variable types and summary statistics\")\n",
    "\n",
    "# potential idea: ask user to for manual input of data question\n",
    "# Provide option for the model to select a data question on its own\n",
    "\n",
    "# gather visualization type from data question\n",
    "prompt_model(f\"what is the best visualization class we should use to characterize this problem?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f920870",
   "metadata": {},
   "source": [
    "## Use VectorDB to query similar examples for few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5127bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts= node['query'], # Chroma will embed this for you\n",
    "    n_results=3 # how many results to return\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b30541-75ca-4647-86b3-4af98d395c31",
   "metadata": {},
   "source": [
    "## Generate the graph's code based on the collected examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7176779e-12a3-4d39-bfc9-91d5f6ae6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "\n",
    "scorer = BERTScorer(model_type='bert-base-uncased')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
